---
id: resuming-paused-crawl
title: Resuming a paused crawl
---

import ApiLink from '@site/src/components/ApiLink';
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';

import ResumeCrawl from '!!raw-loader!roa-loader!./code_examples/resuming_paused_crawl.py';

This example demonstrates how to resume crawling from its last state when running locally, if for some reason it was unexpectedly terminated.

As a foundation, we use <ApiLink to="class/BeautifulSoupCrawler">`BeautifulSoupCrawler`</ApiLink> and this [run example](./beautifulsoup-crawler).

If each run should continue crawling from the previous state, you can configure this using `purge_on_start` in <ApiLink to="class/Configuration">`Configuration`</ApiLink>.

The following code will crawl about 10 links per run, which is controlled by the `max_requests_per_crawl` parameter in <ApiLink to="class/BasicCrawlerOptions">`BasicCrawlerOptions`</ApiLink>, but each time it will continue from where it stopped in the previous run.

<RunnableCodeBlock className="language-python" language="python">
    {ResumeCrawl}
</RunnableCodeBlock>

Alternatively, use the environment variable `CRAWLEE_PURGE_ON_START=0`.

For example, when running code:

```bash
CRAWLEE_PURGE_ON_START=0 python -m best_crawler
```
